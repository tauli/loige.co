{"data":{"site":{"siteMetadata":{"title":"Luciano Mammino \"Loige\" - Web developer, entrepreneur, fighter, butterfly maker!","author":"Luciano Mammino","siteUrl":"https://loige.co/","twitterProfile":"loige","disqusShortName":"loige"}},"markdownRemark":{"id":"797f7b9f-2cca-5572-87e4-2a0a08d8398e","timeToRead":6,"headings":[{"value":"Some examples","depth":2},{"value":"The ‚Äúmagic‚Äù ","depth":2},{"value":"Writing to S3 from the standard output","depth":2},{"value":"Using data from S3 as input for other commands","depth":2},{"value":"Pipeline processing of S3 files","depth":2},{"value":"The 5GB caveat","depth":2},{"value":"That‚Äôs all folks","depth":2}],"html":"<p>This article presents a quick tip that will help you deal with the content of files in S3 through the AWS command line in a much faster and simpler way.</p>\n<p>Did you ever want to simply print the content of a file in S3 from your command line and maybe pipe the output to another command? Or maybe, did you ever needed to pipe the standard output of a sequence of commands directly into a file in S3? I had this need multiple times and, before my amazing colleague Paul made me discover the tip I am about to describe here, I was always using intermediary files to keep track of the input and output of S3 files.</p>\n<h2 id=\"some-examples\"><a href=\"#some-examples\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Some examples</h2>\n<p>Let‚Äôs make few practical examples to make this use case easier to grasp.</p>\n<p>Imagine you have a PostgreSQL database containing GeoIP data and you want to dump all the data to a CSV, gzip it and store it an S3 bucket.</p>\n<p>This is how I used to solve this problem:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token comment\"># dump the data from PostgreSQL to a compressed csv</span>\npsql -U user -d db_name -c <span class=\"token string\">\"Copy (Select * From geoip_v4) To STDOUT With CSV HEADER DELIMITER ',';\"</span> <span class=\"token operator\">|</span> <span class=\"token function\">gzip</span> <span class=\"token operator\">></span> geoip_v4_data.csv.gz\n<span class=\"token comment\"># upload the resulting file to S3</span>\naws s3 <span class=\"token function\">cp</span> geoip_v4_data.csv.gz s3://my-amazing-bucket/geoip_v4_data.csv.gz</code></pre></div>\n<p>At some point in the future, you probably want to read the file from S3 and search for a given CIDR in the content of the file. Again, this is how I would have solved this problem:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token comment\"># download the file from S3</span>\naws s3 <span class=\"token function\">cp</span> s3://my-amazing-bucket/geoip_v4_data.csv.gz <span class=\"token keyword\">.</span>\n<span class=\"token comment\"># decompress the file and search inside it</span>\ngunzip -c geoip_v4_data.csv.gz <span class=\"token operator\">|</span> <span class=\"token function\">grep</span> <span class=\"token string\">\"1.0.8.0/21\"</span></code></pre></div>\n<p>In both cases, I am creating intermediary files and, as you probably already know, this is not ideal for many reasons. Just to name few, this is a slower operation (not fully stream-able), it takes extra space on disk (imagine you have to deal with very big files), finally, it also needs an extra command. Wouldn‚Äôt it be great if we could solve both problems by writing a single pipeline of commands?</p>\n<h2 id=\"the-magic-code-classlanguage-text-code-option-in-code-classlanguage-textaws-cpcode\"><a href=\"#the-magic-code-classlanguage-text-code-option-in-code-classlanguage-textaws-cpcode\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>The ‚Äúmagic‚Äù <code class=\"language-text\">-</code> option in <code class=\"language-text\">aws cp</code></h2>\n<p>Buried at the very bottom of the <code class=\"language-text\">aws s3 cp</code> command help you might (by accident) find this:</p>\n<div class=\"gatsby-highlight\" data-language=\"plain\"><pre class=\"language-plain\"><code class=\"language-plain\">Uploading a local file stream to S3\n\n  WARNING:: PowerShell may alter the encoding of or add a CRLF  to  piped\n  input.\n\n  The  following  cp  command  uploads  a local file stream from standard\n  input to a specified bucket and key:\n\n    aws s3 cp - s3://mybucket/stream.txt\n\nDownloading an S3 object as a local file stream\n\n  WARNING:: PowerShell may alter the encoding of or add a CRLF  to  piped\n  or redirected output.\n\n  The  following cp command downloads an S3 object locally as a stream to\n  standard output. Downloading as a stream is  not  currently  compatible\n  with the --recursive parameter:\n\n    aws s3 cp s3://mybucket/stream.txt -</code></pre></div>\n<p>To make it simple, when running <code class=\"language-text\">aws s3 cp</code> you can use the special argument <code class=\"language-text\">-</code> to indicate the content of the standard input or the content of the standard output (depending on where you put the special argument).</p>\n<h2 id=\"writing-to-s3-from-the-standard-output\"><a href=\"#writing-to-s3-from-the-standard-output\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Writing to S3 from the standard output</h2>\n<p>Using this newly acquired piece of knowledge, we now know we can do something like this to write content from the standard output directly to a file in S3:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">cat</span> <span class=\"token string\">\"hello world\"</span> <span class=\"token operator\">|</span> aws s3 <span class=\"token function\">cp</span> - s3://some-bucket/hello.txt</code></pre></div>\n<p>This way we can rewrite the solution to our first problem as follows:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">psql -U user -d db_name -c <span class=\"token string\">\"Copy (Select * From geoip_v4) To STDOUT With CSV HEADER DELIMITER ',';\"</span> <span class=\"token operator\">|</span> <span class=\"token function\">gzip</span> <span class=\"token operator\">|</span> aws s3 <span class=\"token function\">cp</span> - s3://my-amazing-bucket/geoip_v4_data.csv.gz</code></pre></div>\n<p>This time no intermediary file is created and the data from the gzipped file is immediately streamed to S3 as soon as the first bytes start to be available.</p>\n<h2 id=\"using-data-from-s3-as-input-for-other-commands\"><a href=\"#using-data-from-s3-as-input-for-other-commands\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Using data from S3 as input for other commands</h2>\n<p>The magic <code class=\"language-text\">-</code> argument can be used also to read the content of files in s3 and pass it in the standard output, for instance, you could do the following:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">aws s3 <span class=\"token function\">cp</span> s3://some-bucket/hello.txt -</code></pre></div>\n<p>This will output:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">hello world</code></pre></div>\n<p>Let‚Äôs use this option to rewrite the solution to our second problem as a one-liner:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">aws s3 <span class=\"token function\">cp</span> s3://my-amazing-bucket/geoip_v4_data.csv.gz - <span class=\"token operator\">|</span> gunzip -c geoip_v4_data.csv.gz <span class=\"token operator\">|</span> <span class=\"token function\">grep</span> <span class=\"token string\">\"1.0.8.0/21\"</span></code></pre></div>\n<p>This approach looks much similar to what you would do with a local file and makes integrating other commands seamless with the content of files available in your S3 storage.</p>\n<h2 id=\"pipeline-processing-of-s3-files\"><a href=\"#pipeline-processing-of-s3-files\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Pipeline processing of S3 files</h2>\n<p>We can combine the learnings from the previous two sections to build processing pipelines for S3 files.</p>\n<p>Just to give you a practical example, imagine you have to optimize a png image available in an S3 bucket and save the resulting image in a new bucket.</p>\n<p>To optimize an image we can use <a href=\"https://github.com/imagemin/imagemin-cli\"><code class=\"language-text\">imagemin</code></a> which accepts an image in the standard input and returns the optimized image content through the standard output.</p>\n<p>Assuming we have our source image in <code class=\"language-text\">s3://my-images/image.png</code> and we want to save the optimized version in <code class=\"language-text\">s3://my-images-optimized/image.png</code>, we can write the pipeline as follows:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">aws s3 <span class=\"token function\">cp</span> s3://my-images/image.png - <span class=\"token operator\">|</span> imagemin <span class=\"token operator\">|</span> aws s3 <span class=\"token function\">cp</span> - s3://my-images-optimized/image.png</code></pre></div>\n<p>What will happen behind the scene with this pipeline of commands is the following:</p>\n<ol>\n<li>S3 will start to stream the binary content of <code class=\"language-text\">s3://my-images/image.png</code> to the standard output</li>\n<li>The standard output is then piped to <code class=\"language-text\">imagemin</code> and used as input stream</li>\n<li><code class=\"language-text\">imagemin</code> will start immediately to process the stream and produce an output stream representing the optimized image</li>\n<li>This output stream is then piped to the AWS CLI again and the <code class=\"language-text\">s3 cp</code> command will start to write it to the destination bucket.</li>\n</ol>\n<p>No intermediary file is created in the executing machine and the content is just kept in memory in a streaming fashion during the different phases of the pipeline.</p>\n<h2 id=\"the-5gb-caveat\"><a href=\"#the-5gb-caveat\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>The 5GB caveat</h2>\n<p>If you are writing to S3 files that are bigger than 5GB, you have to use the <code class=\"language-text\">--expected-size</code> option so that AWS CLI can calculate the proper number of parts in the multi-part upload. If you don‚Äôt do this you‚Äôll exceed the number of parts allowed in a multi-part upload and your request will fail.</p>\n<p>From the <a href=\"https://docs.aws.amazon.com/cli/latest/reference/s3/cp.html\">AWS CLI Documentation</a>:</p>\n<blockquote>\n<p><code class=\"language-text\">--expected-size</code> (string): This argument specifies the expected size of a stream in terms of bytes. Note that this argument is needed only when a stream is being uploaded to s3 and the size is larger than 5GB. Failure to include this argument under these conditions may result in a failed upload due to too many parts in the upload.</p>\n</blockquote>\n<p><code class=\"language-text\">--expected-size</code> should be equal or greater than the size of the upload and it doesn‚Äôt have to be perfect. Just close enough.</p>\n<p>(Thanks to mahinka for this suggestion)</p>\n<h2 id=\"thats-all-folks\"><a href=\"#thats-all-folks\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>That‚Äôs all folks</h2>\n<p>I hope this little trick is going be useful to you and that it will allow you to use S3 in a much similar way to how you would use a local file system.</p>\n<p>I am really curious to know what kind of use cases you might come up with, so please, let me know in the comments here if you‚Äôll ever use the nifty <code class=\"language-text\">-</code> option in the <code class=\"language-text\">aws s3 cp</code> command line utility.</p>\n<p>I really look forward to hearing from you!</p>\n<p>Special thanks to Paul for making me discover this trick and to <a href=\"https://www.reddit.com/user/mahinka\">mahinka</a> and <a href=\"https://www.reddit.com/user/paul345\">paul345</a> (on <a href=\"https://www.reddit.com/r/aws/comments/8h73uf/aws_command_line_s3_content_from_stdin_or_to/\">Reddit</a>) for corrections and suggestions.</p>\n<p>Until next time, ciao! üëã</p>","frontmatter":{"title":"AWS Command line: S3 content from stdin or to stdout","slug":"aws-command-line-s3-content-from-stdin-or-to-stdout","author":"Luciano Mammino","tags":["aws","bash","shell"],"date":"May 05, 2018","header_img":{"publicURL":"/static/aws-command-line-s3-content-from-stdin-or-to-stdout-loige-co-luciano-mammino-5a8d5e659e9cbbaf35101f4871e4f445.jpg"}}}},"pageContext":{"slug":"aws-command-line-s3-content-from-stdin-or-to-stdout","width":440,"height":220,"type":"twitter"}}